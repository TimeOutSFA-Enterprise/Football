{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering on NFL Playoff Predictions \n",
    "\n",
    "This post is a part 2 from the NFL Playoff Bound Predictions. In the first post, we touched on feature engineering but in this notebook, we will delve deeper into feature analysis in order to confirm our inital assumption or refine how we can select better features to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # data manipultion librabry\n",
    "import numpy as np # numerical cmputation library\n",
    "\n",
    "# Display up to 120 columns of a dataframe\n",
    "pd.set_option('display.max_columns', 120)\n",
    "\n",
    "import matplotlib.pyplot as plt  # plotting library\n",
    "%matplotlib inline\n",
    "\n",
    "# Internal ipython tool for setting figure size\n",
    "from IPython.core.pylabtools import figsize\n",
    "\n",
    "# Seaborn for visualization\n",
    "import seaborn as sns\n",
    "# sns.set(font_scale = 2)\n",
    "\n",
    "# ML Imports\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import  cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from skll.metrics import spearman\n",
    "\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "team_df = pd.read_csv('nfl_team_stats.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up the styling for the plots in this notebook\n",
    "sns.set(style=\"white\", palette=\"colorblind\", font_scale=1.2, \n",
    "        rc={\"figure.figsize\":(12,9)})\n",
    "RANDOM_STATE = 420\n",
    "N_JOBS=8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test sets\n",
    "train_df = team_df[team_df.TeamID.str.contains('2017') == False]\n",
    "test_df = team_df[team_df.TeamID.str.contains('2017')  == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Best Team Indicators to be Playoff Bound\n",
    "features = ['Pass_O_Total_EPA',\n",
    " 'Run_O_Win_Success_Rate',\n",
    " 'Pass_O_Clutch_EPA_per_Drive',\n",
    " 'Total_Success_Rate',\n",
    " 'Pass_D_Win_Success_Rate',\n",
    " 'Total_EPA',\n",
    " 'Pass_O_Interceptions',\n",
    " 'Pass_O_Success_Rate',\n",
    " 'third_down_conv_rate1',\n",
    " 'fourth_down_conv_rate']\n",
    "\n",
    "# what to predict\n",
    "target = 'playoffs'\n",
    "\n",
    "X = train_df[features].values\n",
    "y = train_df[target].values\n",
    "\n",
    "# the modeling pipeline\n",
    "pipe = Pipeline([(\"imputer\", Imputer()),\n",
    "                 (\"estimator\", RandomForestRegressor(random_state=RANDOM_STATE))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tune our model we will use BayesSearchCV from scikit-optimize, which utilizes bayesian optimization to find the best hyperparameters. We'll use Spearman's rank correlation as our scoring metric since we are mainly concerned with the ranking of teams when it comes to making the playoffs.\n",
    "\n",
    "We tilize spearman's rank-order correlation as a scoring metric. Spearman is a nonparametric version of the Perason correlation. Spearmans correlation coefficient measures the strength and direction associated between two ranked variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use spearman's rank correlation as the scoring metric since\n",
    "# we are concerned with ranking the teams per the features to predict playoff bound\n",
    "spearman_scorer = make_scorer(spearman)\n",
    "\n",
    "# the hyperparamters to search over, including different imputation strategies\n",
    "rf_param_space = {\n",
    "    'imputer__strategy': Categorical(['mean', 'median', 'most_frequent']),\n",
    "    'estimator__max_features': Integer(1, 8),\n",
    "    'estimator__n_estimators': Integer(50, 500), \n",
    "    'estimator__min_samples_split': Integer(2, 200),\n",
    "}\n",
    "# create our search object\n",
    "search = BayesSearchCV(pipe, \n",
    "                      rf_param_space, \n",
    "                      cv=10,\n",
    "                      n_jobs=N_JOBS, \n",
    "                      verbose=0, \n",
    "                      error_score=-9999, \n",
    "                      scoring=spearman_scorer, \n",
    "                      random_state=RANDOM_STATE,\n",
    "                      return_train_score=True, \n",
    "                      n_iter=75)\n",
    "# fit the model\n",
    "# I get some funky warnings, possibly due to the spearman scorer,\n",
    "# I choose to suppress them\n",
    "with warnings.catch_warnings():\n",
    "    warnings.filterwarnings('ignore')\n",
    "    search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best model parameters\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV score\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CV standard deviation\n",
    "search.cv_results_['std_test_score'][search.best_index_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've tuned our model let's evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set data\n",
    "X_test = test_df[features].values\n",
    "y_test = test_df[target].values\n",
    "# predictions\n",
    "y_pred = search.predict(X_test)\n",
    "# evaluation\n",
    "model_test_score = spearman_scorer(search, X_test, y_test)\n",
    "model_test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our model's predictions had a Spearman's rank correlation of about 0.618 Is that good or bad? I'm not sure. To get a better sense of how good or how bad of a score that is we can use the actual playoff teams as a comparison point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance\n",
    "\n",
    "#### Mean Decrease Impurity\n",
    "When using a tree-ensemble like random forest you can find out which features the model found valuable by checking the feature importances. In scikit-learn the feature importances are a reflection of how well a feature reduces some criterion. In our regression example that criterion is mean squared error. This method for calculating feature importance is typically called mean decrease impurity or gini importance.\n",
    "\n",
    "We can access our model's feature importances with the feature_importances_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the estimator and imputer from our pipeline, which will be used\n",
    "# as we try and interpret our model\n",
    "estimator = search.best_estimator_.named_steps['estimator']\n",
    "imputer = search.best_estimator_.named_steps['imputer']\n",
    "\n",
    "estimator.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use eli5's explain_weights_df function, which returns the importances and the feature names we pass it as a pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "\n",
    "# create our dataframe of feature importances\n",
    "feat_imp_df = eli5.explain_weights_df(estimator, feature_names=features)\n",
    "feat_imp_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like Pass_D_W and Run_O Win success rates are important to the model.  A thing to note is that explain_weights_df also returns the standard deviations, but they may not be trustworthy as those values assume a normal distribution. Instead of relying on those standard deviations we can access each tree in our ensemble and plot the full distribution of feature importances.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the feature importances from each tree and then visualize the\n",
    "# distributions as boxplots\n",
    "all_feat_imp_df = pd.DataFrame(data=[tree.feature_importances_ for tree in \n",
    "                                     estimator],\n",
    "                               columns=features)\n",
    "\n",
    "(sns.boxplot(data=all_feat_imp_df)\n",
    "        .set(title='Feature Importance Distributions',\n",
    "             ylabel='Importance'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Permutation Importance\n",
    "\n",
    "Permutation importances or mean decrease accuracy (MDA) is an alternative to mean decrease impurity that can be applied to any model. The basic idea of permutation importance is to permute the values of each feature and measure how much that permutation negatively impacts the scoring metric (which in our case is the Spearman's rank correlation). This gives us a sense of how our model would perform without that specific feature. All we need to do is calculate permutation importance is use PermutationImportance from eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eli5.sklearn import PermutationImportance\n",
    "\n",
    "# we need to impute the data first before calculating permutation importance\n",
    "train_X_imp = imputer.transform(X)\n",
    "# set up the met-estimator to calculate permutation importance on our training\n",
    "# data\n",
    "perm_train = PermutationImportance(estimator, scoring=spearman_scorer,\n",
    "                                   n_iter=50, random_state=RANDOM_STATE)\n",
    "# fit and see the permuation importances\n",
    "perm_train.fit(train_X_imp, y)\n",
    "eli5.explain_weights_df(perm_train, feature_names=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distributions\n",
    "perm_train_feat_imp_df = pd.DataFrame(data=perm_train.results_,\n",
    "                                      columns=features)\n",
    "(sns.boxplot(data=perm_train_feat_imp_df)\n",
    "        .set(title='Permutation Importance Distributions (training data)',\n",
    "             ylabel='Importance'));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the permutation importances we have some low importances, however, again Pass_D_ Win seems to be an important feature to model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "Here is a list of resources that I found helpful when writing up this post:\n",
    "\n",
    "__General__\n",
    "* [NFL Stats Reference](https://www.pro-football-reference.com/teams/dal/2017.htm)\n",
    "\n",
    "__Feature Engineering__\n",
    "* [Model Interpretability](https://github.com/savvastj/model_interpretability_post/blob/master/03_Model_Interpretability_Blog_Post.ipynb)\n",
    "* [Random Forest Intepretation with Scikit Learn](http://blog.datadive.net/random-forest-interpretation-with-scikit-learn/)\n",
    "\n",
    "\n",
    "__Model Implementation and Anlysis__\n",
    "* [Sklearn Metrics](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics)\n",
    "* [Brest Cancer Prediction](https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/breast_cancer_predict.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
